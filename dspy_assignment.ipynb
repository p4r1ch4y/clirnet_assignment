{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsstIo6m1td+q3R4EJn0wL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p4r1ch4y/clirnet_assignment/blob/main/dspy_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DSPy Assignment\n",
        "\n",
        "\n",
        "Time  : 5/11/25 5:30pm\n",
        "\n",
        "Subrata Choudhury - Backend Engineer - Round 1"
      ],
      "metadata": {
        "id": "52J3Dz1gIQXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. install all the dependencies**"
      ],
      "metadata": {
        "id": "GmG42wqbBXvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADN_QPn8EOl4",
        "outputId": "a1dfa7c3-35e0-4b2a-aac4-c3cabeb516b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy-ai\n",
            "  Downloading dspy_ai-3.0.3-py3-none-any.whl.metadata (285 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Collecting dspy>=3.0.3 (from dspy-ai)\n",
            "  Downloading dspy-3.0.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Collecting backoff>=2.2 (from dspy>=3.0.3->dspy-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (1.5.2)\n",
            "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (1.109.1)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (2024.11.6)\n",
            "Requirement already satisfied: orjson>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (3.11.4)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (4.67.1)\n",
            "Collecting optuna>=3.4.0 (from dspy>=3.0.3->dspy-ai)\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (2.11.10)\n",
            "Collecting magicattr>=0.1.6 (from dspy>=3.0.3->dspy-ai)\n",
            "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting litellm>=1.64.0 (from dspy>=3.0.3->dspy-ai)\n",
            "  Downloading litellm-1.79.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting diskcache>=5.6.0 (from dspy>=3.0.3->dspy-ai)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting json-repair>=0.30.0 (from dspy>=3.0.3->dspy-ai)\n",
            "  Downloading json_repair-0.52.5-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (8.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (4.11.0)\n",
            "Collecting asyncer==0.0.8 (from dspy>=3.0.3->dspy-ai)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (3.1.2)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (13.9.4)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (3.6.0)\n",
            "Collecting gepa==0.0.7 (from gepa[dspy]==0.0.7->dspy>=3.0.3->dspy-ai)\n",
            "  Downloading gepa-0.0.7-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy>=3.0.3->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (8.3.0)\n",
            "Collecting fastuuid>=0.13.0 (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai)\n",
            "  Downloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (4.25.1)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.2.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.12.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.22.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.3->dspy-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.3->dspy-ai) (0.11.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (1.17.1)\n",
            "Collecting colorlog (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (2.0.44)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy>=3.0.3->dspy-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy>=3.0.3->dspy-ai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy>=3.0.3->dspy-ai) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy>=3.0.3->dspy-ai) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy>=3.0.3->dspy-ai) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.22.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (1.3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.28.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy>=3.0.3->dspy-ai) (0.1.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.2.0)\n",
            "Downloading dspy_ai-3.0.3-py3-none-any.whl (1.1 kB)\n",
            "Downloading dspy-3.0.3-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading gepa-0.0.7-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.52.5-py3-none-any.whl (26 kB)\n",
            "Downloading litellm-1.79.1-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: magicattr, json-repair, gepa, fastuuid, diskcache, colorlog, backoff, asyncer, optuna, litellm, dspy, dspy-ai\n",
            "Successfully installed asyncer-0.0.8 backoff-2.2.1 colorlog-6.10.1 diskcache-5.6.3 dspy-3.0.3 dspy-ai-3.0.3 fastuuid-0.14.0 gepa-0.0.7 json-repair-0.52.5 litellm-1.79.1 magicattr-0.1.6 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dspy-ai requests beautifulsoup4 pandas lxml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2. import***"
      ],
      "metadata": {
        "id": "Lz8D1laqBUK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "2qUFUbXOEX_J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Add API Keys"
      ],
      "metadata": {
        "id": "g16vNOneINEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=\"redacted\"\n",
        "main_lm = dspy.LM(\"openai/LongCat-Flash-Chat\", api_key=API_KEY, api_base=\"https://api.longcat.chat/openai/v1\")\n",
        "\n",
        "dspy.settings.configure(lm=main_lm, adapter=dspy.XMLAdapter())\n",
        "print(\"DSPy set up! Hope the key works.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VqjOr-2Efit",
        "outputId": "1a2036c1-ff70-490b-b5f0-3f6993aa77ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy set up! Hope the key works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. urls to be scraped\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bXnqf5rbBgQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
        "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
        "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
        "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
        "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
        "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
        "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
        "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "2LJ42tbYI3dG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. dummy user agent to avoid block"
      ],
      "metadata": {
        "id": "TVCgxAd-KXtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# WEB SCRAPER AGENT With Logging MODULE\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def get_text_from_url(url):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):\n",
        "            tag.decompose()\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        text = ' '.join(text.split())\n",
        "        if len(text) > 4000:\n",
        "            text = text[:4000] + \" ... (text cut off to fit)\"\n",
        "        return text, None\n",
        "    except Exception as e:\n",
        "        return \"\", str(e)\n"
      ],
      "metadata": {
        "id": "MIRvnpwbKGLj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 2. ENTITY EXTRACTION with Deduplication and Relation Models MODULE\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class EntityWithAttr(BaseModel):\n",
        "    entity: str = Field(description=\"the named entity\")\n",
        "    attr_type: str = Field(description=\"semantic type of the entity (e.g. Drug, Disease, etc.)\")\n",
        "\n",
        "class ExtractEntities(dspy.Signature):\n",
        "    paragraph: str = dspy.InputField(desc=\"input paragraph\")\n",
        "    entities: List[EntityWithAttr] = dspy.OutputField(desc=\"list of entities and their attr types\")\n",
        "\n",
        "extractor = dspy.Predict(ExtractEntities)\n",
        "\n",
        "class DeduplicateEntities(dspy.Signature):\n",
        "    items: List[EntityWithAttr] = dspy.InputField(desc=\"batch of entities to deduplicate\")\n",
        "    deduplicated: List[EntityWithAttr] = dspy.OutputField(desc=\"deduplicated list\")\n",
        "    confidence: float = dspy.OutputField(desc=\"confidence (0-1) that every item is distinct\")\n",
        "\n",
        "dedup_predictor = dspy.ChainOfThought(DeduplicateEntities)\n",
        "\n",
        "def deduplicate_with_lm(items: List[EntityWithAttr], batch_size:int=10, target_confidence:float=0.9):\n",
        "    if not items: return []\n",
        "    def _process_batch(batch):\n",
        "        while True:\n",
        "            pred = dedup_predictor(items=batch)\n",
        "            if pred.confidence >= target_confidence:\n",
        "                return pred.deduplicated\n",
        "    results = []\n",
        "    for i in range(0, len(items), batch_size):\n",
        "        batch = items[i : i + batch_size]\n",
        "        results.extend(_process_batch(batch))\n",
        "    return results\n",
        "\n",
        "class Relation(BaseModel):\n",
        "    subj: str = Field(description=\"subject entity (exact string as in deduplicated list)\")\n",
        "    pred: str = Field(description=\"short predicate / relation\")\n",
        "    obj: str = Field(description=\"object entity (exact string as in deduplicated list)\")\n",
        "\n",
        "class ExtractRelations(dspy.Signature):\n",
        "    paragraph: str = dspy.InputField(desc=\"original paragraph\")\n",
        "    entities: List[str] = dspy.InputField(desc=\"list of deduplicated entity strings\")\n",
        "    relations: List[Relation] = dspy.OutputField(desc=\"list of subject-predicate-object triples\")\n",
        "\n",
        "rel_predictor = dspy.ChainOfThought(ExtractRelations)\n"
      ],
      "metadata": {
        "id": "zq-DtqxlKuxL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. convert data to mermaid"
      ],
      "metadata": {
        "id": "6QZG7cTzK_jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 5. MERMAID DIAGRAM GENERATOR\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def triples_to_mermaid(triples: list, entity_list: list, max_label_len: int = 40) -> str:\n",
        "    \"\"\"\n",
        "    Convert triples to a VALID Mermaid flowchart LR diagram.\n",
        "    \"\"\"\n",
        "    entity_set = {e.strip().lower() for e in entity_list}\n",
        "    lines = [\"flowchart LR\"]\n",
        "\n",
        "    def _make_id(s: str) -> str:\n",
        "        # Create valid Mermaid node ID (no spaces or special chars)\n",
        "        return s.strip().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\")\n",
        "\n",
        "    for t in triples:\n",
        "        subj_norm, obj_norm = t.subj.strip().lower(), t.obj.strip().lower()\n",
        "        if obj_norm in entity_set:\n",
        "            src, dst, lbl = t.subj, t.obj, t.pred\n",
        "        elif subj_norm in entity_set:\n",
        "            src, dst, lbl = t.obj, t.subj, t.pred\n",
        "        else:\n",
        "            continue\n",
        "        lbl = lbl.strip()\n",
        "        if len(lbl) > max_label_len:\n",
        "            lbl = lbl[:max_label_len - 3] + \"...\"\n",
        "        src_id, dst_id = _make_id(src), _make_id(dst)\n",
        "        lines.append(f'    {src_id}[\"{src}\"] -->|{lbl}| {dst_id}[\"{dst}\"]')\n",
        "    return \"\\n\".join(lines)"
      ],
      "metadata": {
        "id": "zEVM1YOBK1KC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 9. MAIN PROCESSING and File Generation Logging\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "process_logs = []\n",
        "\n",
        "def process_url(url, index):\n",
        "    log = {'url': url, 'index': index+1, 'scrape': '', 'entities': '', 'relations': '', 'mermaid_save': ''}\n",
        "    paragraph, scrape_err = get_text_from_url(url)\n",
        "    if not paragraph:\n",
        "        mermaid_code = \"flowchart LR\\n    No_data[\\\"No text scraped - empty graph\\\"]\"\n",
        "        filename = f'mermaid_{index+1:02d}.md'\n",
        "        try:\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"```mermaid\\n{mermaid_code}\\n```\")\n",
        "            log['mermaid_save'] = 'OK (empty graph)'\n",
        "        except Exception as e:\n",
        "            log['mermaid_save'] = f'ERROR {e}'\n",
        "        log['scrape'] = f'ERROR {scrape_err}'\n",
        "        process_logs.append(log)\n",
        "        return {'url': url, 'entities': [], 'relations': [], 'mermaid': mermaid_code, 'log': log}\n",
        "    log['scrape'] = 'OK'\n",
        "\n",
        "    time.sleep(2)\n",
        "    try:\n",
        "        extracted = extractor(paragraph=paragraph)\n",
        "        if not extracted.entities:\n",
        "            log['entities'] = 'ERROR (zero extracted)'\n",
        "        else:\n",
        "            log['entities'] = f'OK ({len(extracted.entities)})'\n",
        "    except Exception as e:\n",
        "        extracted = None\n",
        "        log['entities'] = f'ERROR {e}'\n",
        "\n",
        "    if not extracted or not extracted.entities:\n",
        "        mermaid_code = \"flowchart LR\\n    No_entities[\\\"No entities - empty graph\\\"]\"\n",
        "        filename = f'mermaid_{index+1:02d}.md'\n",
        "        try:\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"```mermaid\\n{mermaid_code}\\n```\")\n",
        "            log['mermaid_save'] = 'OK (empty graph)'\n",
        "        except Exception as e:\n",
        "            log['mermaid_save'] = f'ERROR {e}'\n",
        "        process_logs.append(log)\n",
        "        return {'url': url, 'entities': [], 'relations': [], 'mermaid': mermaid_code, 'log': log}\n",
        "\n",
        "    unique = deduplicate_with_lm(extracted.entities, batch_size=10, target_confidence=0.9)\n",
        "    entity_strings = [e.entity for e in unique]\n",
        "\n",
        "    try:\n",
        "        rel_out = rel_predictor(paragraph=paragraph, entities=entity_strings)\n",
        "        rels_ok = len(rel_out.relations) if rel_out else 0\n",
        "        log['relations'] = f'OK ({rels_ok})'\n",
        "    except Exception as e:\n",
        "        rel_out = None\n",
        "        log['relations'] = f'ERROR {e}'\n",
        "\n",
        "    relations = rel_out.relations if rel_out else []\n",
        "\n",
        "    mermaid_code = triples_to_mermaid(relations, entity_strings)\n",
        "    filename = f'mermaid_{index+1:02d}.md'\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"```mermaid\\n{mermaid_code}\\n```\")\n",
        "        log['mermaid_save'] = 'OK'\n",
        "    except Exception as e:\n",
        "        log['mermaid_save'] = f'ERROR {e}'\n",
        "    process_logs.append(log)\n",
        "\n",
        "    return {\n",
        "        'url': url,\n",
        "        'entities': unique,\n",
        "        'relations': relations,\n",
        "        'mermaid': mermaid_code,\n",
        "        'log': log\n",
        "    }"
      ],
      "metadata": {
        "id": "JzMrU6hVLuCU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **Process URLS and Generate FILES**(*Wait a bit as it's scapes and processes and call the functions*)"
      ],
      "metadata": {
        "id": "R1miWBDGCem6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = []\n",
        "for i, url in enumerate(urls):\n",
        "    try:\n",
        "        result = process_url(url, i)\n",
        "        all_results.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"Unhandled error on URL {i+1}: {e}\")\n"
      ],
      "metadata": {
        "id": "dI1M0230MSh0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CSV FILE Generation**"
      ],
      "metadata": {
        "id": "iMdF9nl6MiyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 7. CSV FILE GENERATION and ERROR LOGGING MODULE\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "csv_rows = []\n",
        "for result in all_results:\n",
        "    url = result['url']\n",
        "    for ent in result['entities']:\n",
        "        csv_rows.append({\n",
        "            'link': url,\n",
        "            'tag': ent.entity,\n",
        "            'tag_type': ent.attr_type\n",
        "        })\n",
        "\n",
        "csv_status = \"\"\n",
        "df = pd.DataFrame(csv_rows)\n",
        "if not df.empty:\n",
        "    try:\n",
        "        df = df.drop_duplicates()\n",
        "        df.to_csv('tags.csv', index=False)\n",
        "        csv_status = f'OK ({len(df)} rows)'\n",
        "    except Exception as e:\n",
        "        csv_status = f'ERROR {e}'\n",
        "\n",
        "\n",
        "    csv_content = df.to_csv(index=False)\n",
        "    md_content = f\"# Tags CSV Export\\n\\n```csv\\n{csv_content}```\"\n",
        "    try:\n",
        "        with open('tags.md', 'w', encoding='utf-8') as f:\n",
        "            f.write(md_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving tags.md: {e}\")\n",
        "    print(f\"Saved tags.csv, csv_status: {csv_status}\")\n",
        "else:\n",
        "    csv_status = \"ERROR (nothing to save)\"\n",
        "    with open('tags.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(\"# Tags CSV Export\\n\\nNo data available.\")\n",
        "    print(\"No entities for CSV :(\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAoYdiYlMlTC",
        "outputId": "318fa342-2414-473f-f534-db737b127322"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tags.csv, csv_status: OK (250 rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** 12 scapping and generation summarry **"
      ],
      "metadata": {
        "id": "vwJblpYWMzyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 8. Log Summary\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "df_log = pd.DataFrame(process_logs)\n",
        "print(\"\\nPROCESS LOG SUMMARY:\")\n",
        "print(df_log)\n",
        "\n",
        "df_log.to_csv('process_log.csv', index=False)\n",
        "\n",
        "errors_only = df_log[\n",
        "    (df_log['scrape'].str.startswith('ERROR')) |\n",
        "    (df_log['entities'].str.startswith('ERROR')) |\n",
        "    (df_log['relations'].str.startswith('ERROR')) |\n",
        "    (df_log['mermaid_save'].str.startswith('ERROR'))\n",
        "]\n",
        "print(\"\\nERRORS FOUND:\")\n",
        "print(errors_only if not errors_only.empty else \"No errors, all OK!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQBHfsoQMwup",
        "outputId": "efde4810-4954-4e98-8b32-5458a6cca54f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROCESS LOG SUMMARY:\n",
            "                                                 url  index  \\\n",
            "0  https://en.wikipedia.org/wiki/Sustainable_agri...      1   \n",
            "1  https://www.nature.com/articles/d41586-025-033...      2   \n",
            "2  https://www.sciencedirect.com/science/article/...      3   \n",
            "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...      4   \n",
            "4          https://www.fao.org/3/y4671e/y4671e06.htm      5   \n",
            "5  https://www.medscape.com/viewarticle/time-reco...      6   \n",
            "6  https://www.sciencedirect.com/science/article/...      7   \n",
            "7  https://www.frontiersin.org/news/2025/09/01/re...      8   \n",
            "8  https://www.medscape.com/viewarticle/second-do...      9   \n",
            "9  https://www.theguardian.com/global-development...     10   \n",
            "\n",
            "                                              scrape entities relations  \\\n",
            "0                                                 OK  OK (46)   OK (48)   \n",
            "1                                                 OK  OK (40)   OK (24)   \n",
            "2  ERROR 403 Client Error: Forbidden for url: htt...                      \n",
            "3                                                 OK  OK (29)   OK (33)   \n",
            "4                                                 OK  OK (22)   OK (20)   \n",
            "5                                                 OK  OK (47)   OK (41)   \n",
            "6  ERROR 403 Client Error: Forbidden for url: htt...                      \n",
            "7                                                 OK  OK (20)   OK (24)   \n",
            "8                                                 OK  OK (25)   OK (31)   \n",
            "9                                                 OK  OK (25)   OK (34)   \n",
            "\n",
            "       mermaid_save  \n",
            "0                OK  \n",
            "1                OK  \n",
            "2  OK (empty graph)  \n",
            "3                OK  \n",
            "4                OK  \n",
            "5                OK  \n",
            "6  OK (empty graph)  \n",
            "7                OK  \n",
            "8                OK  \n",
            "9                OK  \n",
            "\n",
            "ERRORS FOUND:\n",
            "                                                 url  index  \\\n",
            "2  https://www.sciencedirect.com/science/article/...      3   \n",
            "6  https://www.sciencedirect.com/science/article/...      7   \n",
            "\n",
            "                                              scrape entities relations  \\\n",
            "2  ERROR 403 Client Error: Forbidden for url: htt...                      \n",
            "6  ERROR 403 Client Error: Forbidden for url: htt...                      \n",
            "\n",
            "       mermaid_save  \n",
            "2  OK (empty graph)  \n",
            "6  OK (empty graph)  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "summary"
      ],
      "metadata": {
        "id": "Y28xRmMNNk5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_entities = sum(len(r['entities']) for r in all_results)\n",
        "total_rels = sum(len(r['relations']) for r in all_results)\n",
        "print(\"=\" * 50)\n",
        "print(\"ASSIGNMENT COMPLETE!\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"URLs processed: {len(urls)}\")\n",
        "print(f\"Total deduplicated entities: {total_entities}\")\n",
        "print(f\"Total relations: {total_rels}\")\n",
        "print(\"Files: tags.csv, tags.md (CSV & Markdown), 10 mermaid_XX.md graphs, process_log.csv (log)\")\n",
        "print(\"Mermaid Diagrams are already embeeded, Paste mermaid codes into mermaid.live if you wish to view code and graphs.\")\n",
        "print(f\"CSV Save Status: {csv_status}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1CiHWpHNmGC",
        "outputId": "cae65309-c21b-441c-bac2-84a698ce0817"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "ASSIGNMENT COMPLETE!\n",
            "==================================================\n",
            "URLs processed: 10\n",
            "Total deduplicated entities: 251\n",
            "Total relations: 255\n",
            "Files: tags.csv, tags.md (CSV & Markdown), 10 mermaid_XX.md graphs, process_log.csv (log)\n",
            "Mermaid Diagrams are already embeeded, Paste mermaid codes into mermaid.live if you wish to view code and graphs.\n",
            "CSV Save Status: OK (250 rows)\n"
          ]
        }
      ]
    }
  ]
}