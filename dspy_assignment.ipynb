{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1vzFtldvDrnSBCk8DWDg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p4r1ch4y/clirnet_assignment/blob/main/dspy_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DSPy Assignment\n",
        "\n",
        "\n",
        "Time  : 5/11/25 5:30pm - 6/11/25 2:30 am\n",
        "\n",
        "Subrata Choudhury - Backend Engineer - Round 1"
      ],
      "metadata": {
        "id": "52J3Dz1gIQXH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADN_QPn8EOl4",
        "outputId": "ec5c6946-d86f-4830-fa15-22436a2a6513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dspy-ai in /usr/local/lib/python3.12/dist-packages (3.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: dspy>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from dspy-ai) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: backoff>=2.2 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (2.2.1)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (1.5.2)\n",
            "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (1.109.1)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (2024.11.6)\n",
            "Requirement already satisfied: orjson>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (3.11.4)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (4.67.1)\n",
            "Requirement already satisfied: optuna>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (4.5.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (2.11.10)\n",
            "Requirement already satisfied: magicattr>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (0.1.6)\n",
            "Requirement already satisfied: litellm>=1.64.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (1.79.1)\n",
            "Requirement already satisfied: diskcache>=5.6.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (5.6.3)\n",
            "Requirement already satisfied: json-repair>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (0.52.4)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (8.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (4.11.0)\n",
            "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (0.0.8)\n",
            "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (3.1.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (13.9.4)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.3->dspy-ai) (3.6.0)\n",
            "Requirement already satisfied: gepa==0.0.7 in /usr/local/lib/python3.12/dist-packages (from gepa[dspy]==0.0.7->dspy>=3.0.3->dspy-ai) (0.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy>=3.0.3->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.13.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (8.3.0)\n",
            "Requirement already satisfied: fastuuid>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.14.0)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (4.25.1)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.2.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.12.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.22.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.3->dspy-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.3->dspy-ai) (0.11.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (1.17.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (6.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (2.0.44)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy>=3.0.3->dspy-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy>=3.0.3->dspy-ai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy>=3.0.3->dspy-ai) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy>=3.0.3->dspy-ai) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy>=3.0.3->dspy-ai) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.22.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (1.3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.28.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy>=3.0.3->dspy-ai) (0.1.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.3->dspy-ai) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install dspy-ai requests beautifulsoup4 pandas lxml\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import dspy\n",
        "import copy\n",
        "from typing import List, Optional\n",
        "from typing import Literal, Dict, Union\n",
        "from dspy.adapters import XMLAdapter\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "2qUFUbXOEX_J"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add API Keys"
      ],
      "metadata": {
        "id": "g16vNOneINEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=\"Try_ur__API_get_api_from_longcat.chat\"\n",
        "main_lm = dspy.LM(\"openai/LongCat-Flash-Chat\", api_key=API_KEY, api_base=\"https://api.longcat.chat/openai/v1\")\n",
        "\n",
        "dspy.settings.configure(lm=main_lm, adapter=dspy.XMLAdapter())\n",
        "print(\"DSPy set up! Hope the key works.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VqjOr-2Efit",
        "outputId": "5c2b978b-755e-4a51-e234-ef8bef0f69aa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy set up! Hope the key works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# urls to scrap from the problem doc\n",
        "urls = [\n",
        "\n",
        "\"https://en.wikipedia.org/wiki/Sustainable_agriculture \",\n",
        "\"https://www.nature.com/articles/d41586-025-03353-5 \",\n",
        "\"https://www.sciencedirect.com/science/article/pii/S1043661820315152 \",\n",
        "\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/ \",\n",
        "\"https://www.fao.org/3/y4671e/y4671e06.htm \",\n",
        "\"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria \",\n",
        "\"https://www.sciencedirect.com/science/article/pii/S0378378220307088 \",\n",
        "\"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets \",\n",
        "\"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7 \",\n",
        "\"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india \",\n",
        "\n",
        "]\n",
        "print(f\"Got {len(urls)} URLs ready to scrape and process.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LJ42tbYI3dG",
        "outputId": "31098843-37d2-4ad3-8bbf-5b843beabc31"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 10 URLs ready to scrape and process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dummy user agent to avoid block"
      ],
      "metadata": {
        "id": "TVCgxAd-KXtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_from_url(url):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):\n",
        "            tag.decompose()\n",
        "\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        text = ' '.join(text.split())\n",
        "        if len(text) > 4000:\n",
        "            text = text[:4000] + \" ... (text cut off to fit)\"\n",
        "\n",
        "        print(f\"Got {len(text)} chars from {url.split('/')[2]}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Scrape failed for {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "test_url = urls[0]\n",
        "test_text = get_text_from_url(test_url)\n",
        "if test_text:\n",
        "    print(\"Sample text start:\", test_text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIRvnpwbKGLj",
        "outputId": "a0f5c9e7-06ea-4325-c412-506e7bbedee3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 4026 chars from en.wikipedia.org\n",
            "Sample text start: Sustainable agriculture is farming in sustainable ways meeting society's present food and textile ne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity extractor`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "collapsed": true,
        "id": "dP-vACw9K2bz",
        "outputId": "95b27f21-a27d-46f9-b6a3-a8150e267758"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-4000180668.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4000180668.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    entity extractor`\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Tuple\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class EntityWithAttr(BaseModel):\n",
        "    entity: str = Field(description=\"the named entity\")\n",
        "    attr_type: str = Field(description=\"semantic type of the entity (e.g. Drug, Disease, Symptom, etc.)\")\n",
        "\n",
        "class ExtractEntities(dspy.Signature):\n",
        "    \"\"\"From the paragraph extract all relevant entities and their semantic attribute types.\"\"\"\n",
        "    paragraph: str = dspy.InputField(desc=\"input paragraph\")\n",
        "    entities: List[EntityWithAttr] = dspy.OutputField(desc=\"list of entities and their attribute types\")\n",
        "\n",
        "extractor = dspy.Predict(ExtractEntities)\n",
        "print(\"Entity extractor copied from sample.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq-DtqxlKuxL",
        "outputId": "b90e2a68-2005-4362-d53c-cc8eefd9ef47"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity extractor copied from sample.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "deduplication"
      ],
      "metadata": {
        "id": "6QZG7cTzK_jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 2.  DEDUPLICATOR (recursive batching + confidence loop)\n",
        "# ---------------------------------------------------------\n",
        "class DeduplicateEntities(dspy.Signature):\n",
        "    \"\"\"Given a list of (entity, attr_type) decide which ones are duplicates.\n",
        "    Return a deduplicated list and a confidence that the remaining items are ALL distinct.\"\"\"\n",
        "    items: List[EntityWithAttr] = dspy.InputField(desc=\"batch of entities to deduplicate\")\n",
        "    deduplicated: List[EntityWithAttr] = dspy.OutputField(desc=\"deduplicated list\")\n",
        "    confidence: float = dspy.OutputField(\n",
        "        desc=\"confidence (0-1) that every item in deduplicated is semantically distinct\"\n",
        "    )\n",
        "\n",
        "dedup_predictor = dspy.ChainOfThought(DeduplicateEntities)\n",
        "\n",
        "def deduplicate_with_lm(\n",
        "    items: List[EntityWithAttr],\n",
        "    *,\n",
        "    batch_size: int = 10,\n",
        "    target_confidence: float = 0.9,\n",
        ") -> List[EntityWithAttr]:\n",
        "    \"\"\"\n",
        "    Recursively deduplicate using the LM.\n",
        "    Works by:\n",
        "    1. splitting into batches of `batch_size`\n",
        "    2. for each batch asking the LM for duplicates + confidence\n",
        "    3. rerunning the batch until confidence >= target_confidence\n",
        "    4. concatenating results from all batches\n",
        "    \"\"\"\n",
        "    if not items:\n",
        "        return []\n",
        "\n",
        "    # helper to process one batch\n",
        "    def _process_batch(batch: List[EntityWithAttr]) -> List[EntityWithAttr]:\n",
        "        while True:\n",
        "            pred = dedup_predictor(items=batch)\n",
        "            if pred.confidence >= target_confidence:\n",
        "                return pred.deduplicated\n",
        "            # otherwise loop again with same batch\n",
        "\n",
        "    # split into batches and process\n",
        "    results = []\n",
        "    for i in range(0, len(items), batch_size):\n",
        "        batch = items[i : i + batch_size]\n",
        "        results.extend(_process_batch(batch))\n",
        "    return results\n",
        "\n",
        "print(\"Deduplication function from sample ready. Uses loops!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEVM1YOBK1KC",
        "outputId": "04568977-a7e8-4db8-9af6-17a7678955ee"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deduplication function from sample ready. Uses loops!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "#  RELATION EXTRACTION\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class Relation(BaseModel):\n",
        "    subj: str = Field(description=\"subject entity (exact string as in deduplicated list)\")\n",
        "    pred: str = Field(description=\"short predicate / relation phrase\")\n",
        "    obj: str = Field(description=\"object entity (exact string as in deduplicated list)\")\n",
        "\n",
        "class ExtractRelations(dspy.Signature):\n",
        "    \"\"\"Given the original paragraph and a list of unique entities, extract all factual (subject, predicate, object) triples that are explicitly stated or clearly implied.\"\"\"\n",
        "    paragraph: str = dspy.InputField(desc=\"original paragraph\")\n",
        "    entities: List[str] = dspy.InputField(desc=\"list of deduplicated entity strings\")\n",
        "    relations: List[Relation] = dspy.OutputField(desc=\"list of subject-predicate-object triples\")\n",
        "\n",
        "rel_predictor = dspy.ChainOfThought(ExtractRelations)\n",
        "print(\"Relation extractor from sample.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzMrU6hVLuCU",
        "outputId": "f3f79a35-e4bd-4d7f-83c8-daf07a5e9e17"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relation extractor from sample.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "#   MERMAID SERIALISER  (revised)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def triples_to_mermaid(\n",
        "    triples: list[Relation],\n",
        "    entity_list: list[str],\n",
        "    max_label_len: int = 40\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Convert triples to a VALID Mermaid flowchart LR diagram.\n",
        "    \"\"\"\n",
        "    entity_set = {e.strip().lower() for e in entity_list}\n",
        "    lines = [\"flowchart LR\"]\n",
        "\n",
        "    def _make_id(s: str) -> str:\n",
        "      # Create valid Mermaid node ID (no spaces or special chars)\n",
        "        return s.strip().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\")\n",
        "\n",
        "    for t in triples:\n",
        "        subj_norm, obj_norm = t.subj.strip().lower(), t.obj.strip().lower()\n",
        "\n",
        "        if obj_norm in entity_set:\n",
        "            src, dst, lbl = t.subj, t.obj, t.pred\n",
        "        elif subj_norm in entity_set:\n",
        "            src, dst, lbl = t.obj, t.subj, t.pred\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Sanitize label\n",
        "        lbl = lbl.strip()\n",
        "        if len(lbl) > max_label_len:\n",
        "            lbl = lbl[:max_label_len - 3] + \"...\"\n",
        "\n",
        "        # Use valid IDs with display labels\n",
        "        src_id, dst_id = _make_id(src), _make_id(dst)\n",
        "        lines.append(f' {src_id}[\"{src}\"] -->|{lbl}| {dst_id}[\"{dst}\"]')\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "print(\"Mermaid function from sample. Makes graphs!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI1M0230MSh0",
        "outputId": "181df093-be4d-467c-aa8b-4a3344fe0ece"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mermaid function from sample. Makes graphs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "main funtion"
      ],
      "metadata": {
        "id": "iMdF9nl6MiyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_url(url, index):\n",
        "    print(f\"\\n--- Doing URL {index+1}/10: {url} ---\")\n",
        "\n",
        "    paragraph = get_text_from_url(url)\n",
        "    if not paragraph:\n",
        "        return {'url': url, 'entities': [], 'relations': [], 'mermaid': 'No text'}\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "    extracted = extractor(paragraph=paragraph)\n",
        "    print(f\"Found {len(extracted.entities)} raw entities\")\n",
        "\n",
        "    unique = deduplicate_with_lm(extracted.entities, batch_size=10, target_confidence=0.9)\n",
        "    entity_strings = [e.entity for e in unique]\n",
        "    print(f\"After dedup: {len(unique)} unique\")\n",
        "\n",
        "    rel_out = rel_predictor(paragraph=paragraph, entities=entity_strings)\n",
        "    print(f\"Found {len(rel_out.relations)} relations\")\n",
        "\n",
        "    mermaid_code = triples_to_mermaid(rel_out.relations, entity_strings)\n",
        "\n",
        "    filename = f'mermaid_{index+1:02d}.md'\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(mermaid_code)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "    return {\n",
        "        'url': url,\n",
        "        'entities': unique,\n",
        "        'relations': rel_out.relations,\n",
        "        'mermaid': mermaid_code\n",
        "    }\n",
        "\n",
        "print(\"My process function ready. Uses sample code inside.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAoYdiYlMlTC",
        "outputId": "99a8bf60-d68e-41c2-de7f-4cd8c3b6c2eb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My process function ready. Uses sample code inside.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run on all URLS with loop**"
      ],
      "metadata": {
        "id": "vwJblpYWMzyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = []\n",
        "for i, url in enumerate(urls):\n",
        "    try:\n",
        "        result = process_url(url, i)\n",
        "        all_results.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error on URL {i+1}: {e}\")\n",
        "        all_results.append({'url': url, 'entities': [], 'relations': [], 'mermaid': 'Error'})\n",
        "\n",
        "print(\"\\nAll done! Processed 10 URLs.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQBHfsoQMwup",
        "outputId": "e6783cf4-65a6-4209-8a47-a4fa17a169a9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Doing URL 1/10: https://en.wikipedia.org/wiki/Sustainable_agriculture  ---\n",
            "Got 4026 chars from en.wikipedia.org\n",
            "Found 46 raw entities\n",
            "After dedup: 46 unique\n",
            "Found 50 relations\n",
            "Saved mermaid_01.md\n",
            "\n",
            "--- Doing URL 2/10: https://www.nature.com/articles/d41586-025-03353-5  ---\n",
            "Got 453 chars from www.nature.com\n",
            "Found 0 raw entities\n",
            "After dedup: 0 unique\n",
            "Found 0 relations\n",
            "Saved mermaid_02.md\n",
            "\n",
            "--- Doing URL 3/10: https://www.sciencedirect.com/science/article/pii/S1043661820315152  ---\n",
            "Got 0 chars from www.sciencedirect.com\n",
            "\n",
            "--- Doing URL 4/10: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/  ---\n",
            "Got 373 chars from www.ncbi.nlm.nih.gov\n",
            "Found 8 raw entities\n",
            "After dedup: 7 unique\n",
            "Found 8 relations\n",
            "Saved mermaid_04.md\n",
            "\n",
            "--- Doing URL 5/10: https://www.fao.org/3/y4671e/y4671e06.htm  ---\n",
            "Got 50 chars from www.fao.org\n",
            "Found 0 raw entities\n",
            "After dedup: 0 unique\n",
            "Found 0 relations\n",
            "Saved mermaid_05.md\n",
            "\n",
            "--- Doing URL 6/10: https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria  ---\n",
            "Got 119 chars from www.medscape.com\n",
            "Found 0 raw entities\n",
            "After dedup: 0 unique\n",
            "Found 0 relations\n",
            "Saved mermaid_06.md\n",
            "\n",
            "--- Doing URL 7/10: https://www.sciencedirect.com/science/article/pii/S0378378220307088  ---\n",
            "Got 0 chars from www.sciencedirect.com\n",
            "\n",
            "--- Doing URL 8/10: https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets  ---\n",
            "Got 111 chars from www.frontiersin.org\n",
            "Found 0 raw entities\n",
            "After dedup: 0 unique\n",
            "Found 0 relations\n",
            "Saved mermaid_08.md\n",
            "\n",
            "--- Doing URL 9/10: https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7  ---\n",
            "Got 119 chars from www.medscape.com\n",
            "Found 0 raw entities\n",
            "After dedup: 0 unique\n",
            "Found 0 relations\n",
            "Saved mermaid_09.md\n",
            "\n",
            "--- Doing URL 10/10: https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india  ---\n",
            "Got 0 chars from www.theguardian.com\n",
            "\n",
            "All done! Processed 10 URLs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_rows = []\n",
        "for result in all_results:\n",
        "    url = result['url']\n",
        "    for ent in result['entities']:\n",
        "        csv_rows.append({\n",
        "            'link': url,\n",
        "            'tag': ent.entity,\n",
        "            'tag_type': ent.attr_type\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(csv_rows)\n",
        "if not df.empty:\n",
        "    df = df.drop_duplicates()\n",
        "    df.to_csv('tags.csv', index=False)\n",
        "    print(f\"Saved tags.csv: {len(df)} rows total\")\n",
        "    print(df.head(10))\n",
        "else:\n",
        "    print(\"No entities for CSV :(\")\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"\\nTop types:\")\n",
        "    print(df['tag_type'].value_counts().head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU4-jJr-Ncrf",
        "outputId": "736870dc-cf88-487f-8dc3-fe791b6e21cb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tags.csv: 52 rows total\n",
            "                                                link  \\\n",
            "0  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "1  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "2  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "3  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "4  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "5  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "6  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "7  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "8  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "9  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "\n",
            "                           tag             tag_type  \n",
            "0      Sustainable agriculture              Concept  \n",
            "1                      farming             Activity  \n",
            "2                         food             Resource  \n",
            "3                textile needs             Resource  \n",
            "4           ecosystem services              Concept  \n",
            "5               sustainability              Concept  \n",
            "6  flexible business processes             Practice  \n",
            "7            farming practices             Practice  \n",
            "8      environmental footprint              Concept  \n",
            "9               climate change  Environmental Issue  \n",
            "\n",
            "Top types:\n",
            "tag_type\n",
            "Practice               12\n",
            "Concept                10\n",
            "Certification           7\n",
            "Environmental Issue     6\n",
            "Resource                6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "summary"
      ],
      "metadata": {
        "id": "Y28xRmMNNk5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*50)\n",
        "print(\"ASSIGNMENT COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "total_entities = sum(len(r['entities']) for r in all_results)\n",
        "total_rels = sum(len(r['relations']) for r in all_results)\n",
        "print(f\"URLs: {len(urls)}\")\n",
        "print(f\"Total entities (after dedup): {total_entities}\")\n",
        "print(f\"Total relations: {total_rels}\")\n",
        "print(\"Mermaid files: mermaid_01.md to mermaid_10.md\")\n",
        "print(\"CSV: tags.csv with link, tag, tag_type\")\n",
        "print(\"Paste Mermaids into mermaid.live to see graphs.\")\n",
        "print(\"I copied most from sample as i'm very new, added scrape and loop. Hope it works \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1CiHWpHNmGC",
        "outputId": "89bae6e9-4be0-4216-ae02-0b11ebe47ec0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "ASSIGNMENT COMPLETE!\n",
            "==================================================\n",
            "URLs: 10\n",
            "Total entities (after dedup): 53\n",
            "Total relations: 58\n",
            "Mermaid files: mermaid_01.md to mermaid_10.md\n",
            "CSV: tags.csv with link, tag, tag_type\n",
            "Paste Mermaids into mermaid.live to see graphs.\n",
            "I copied most from sample as i'm very new, added scrape and loop. Hope it works \n"
          ]
        }
      ]
    }
  ]
}